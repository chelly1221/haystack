from fastapi import APIRouter, UploadFile, File, Form, HTTPException, BackgroundTasks
from typing import List, Optional
import os
import shutil
import uuid
import unicodedata
import logging
import json
import asyncio
from task_manager_sqlite import task_manager, TaskStatus

def get_background_upload_router(document_store, embedder):
    router = APIRouter()
    
    # ë°±ê·¸ë¼ìš´ë“œ ì›Œì»¤ ì‹œì‘
    async def start_background_worker():
        """ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì²˜ë¦¬ ì›Œì»¤"""
        while True:
            try:
                # íì—ì„œ ì‘ì—… ê°€ì ¸ì˜¤ê¸° (íƒ€ì„ì•„ì›ƒ ì„¤ì •ìœ¼ë¡œ ì£¼ê¸°ì  ì²´í¬)
                try:
                    task_id = await asyncio.wait_for(
                        task_manager.processing_queue.get(), 
                        timeout=1.0
                    )
                except asyncio.TimeoutError:
                    continue
                
                task = task_manager.get_task(task_id)
                
                if not task:
                    continue
                
                # ì‹¤ì œ ì²˜ë¦¬ í•¨ìˆ˜ í˜¸ì¶œ
                await process_file_task(task_id, document_store, embedder)
                
            except Exception as e:
                logging.error(f"âŒ Background worker error: {e}")
                await asyncio.sleep(1)
    
    # ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ ì›Œì»¤ ì‹¤í–‰
    asyncio.create_task(start_background_worker())
    
    # ì£¼ê¸°ì ì¸ ì •ë¦¬ ì‘ì—…
    async def cleanup_worker():
        while True:
            await asyncio.sleep(3600)  # 1ì‹œê°„ë§ˆë‹¤
            task_manager.cleanup_old_tasks()
    
    asyncio.create_task(cleanup_worker())
    
    @router.post("/upload-async/")
    async def upload_async(
        files: List[UploadFile] = File(...),
        tags: Optional[str] = Form(None),
        sosok: Optional[str] = Form(None),
        site: Optional[str] = Form(None),
        type: Optional[str] = Form(None),
        top_margin: Optional[float] = Form(0.1),
        bottom_margin: Optional[float] = Form(0.1),
        margin_settings: Optional[str] = Form(None),
        overwrite_decisions: Optional[str] = Form(None)
    ):
        """ë¹„ë™ê¸° íŒŒì¼ ì—…ë¡œë“œ - ì¦‰ì‹œ task_id ë°˜í™˜"""
        
        sosok = sosok.strip() if sosok else ""
        site = site.strip() if site else ""
        
        task_ids = []
        created_tasks = []  # ìƒì„±ëœ ì‘ì—… ì •ë³´ ì €ì¥
        
        # ì—…ë¡œë“œ ë””ë ‰í† ë¦¬ í™•ì¸
        os.makedirs("./uploads", exist_ok=True)
        
        for file in files:
            # íŒŒì¼ í™•ì¥ì ê²€ì¦
            ext = os.path.splitext(file.filename.lower())[-1]
            if ext not in [".pdf", ".hwpx", ".docx", ".pptx"]:
                continue
            
            # ì‘ì—… ìƒì„± - ì´ˆê¸° ìƒíƒœëŠ” UPLOADING
            task_id = task_manager.create_task(file.filename, sosok, site)
            task_ids.append(task_id)
            
            # íŒŒì¼ëª… ì •ê·œí™”
            normalized_filename = unicodedata.normalize("NFC", file.filename.strip())
            unique_filename = f"{uuid.uuid4().hex}_{normalized_filename}"
            file_path = os.path.join("./uploads", unique_filename)
            
            try:
                # íŒŒì¼ ì €ì¥ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                task_manager.update_task_status(
                    task_id, 
                    TaskStatus.UPLOADING,
                    progress=50,
                    message="íŒŒì¼ ì €ì¥ ì¤‘..."
                )
                
                # íŒŒì¼ ì¦‰ì‹œ ì €ì¥ (ë™ê¸°ì ìœ¼ë¡œ)
                content = await file.read()
                
                # íŒŒì¼ì„ ë™ê¸°ì ìœ¼ë¡œ ì €ì¥
                with open(file_path, "wb") as f:
                    f.write(content)
                
                # íŒŒì¼ ê²½ë¡œ ì €ì¥
                task_manager.set_task_file_path(task_id, file_path)
                
                # ë©”íƒ€ë°ì´í„° íŒŒì¼ë¡œ ì €ì¥
                meta_path = file_path + ".meta.json"
                metadata = {
                    "task_id": task_id,
                    "file_path": file_path,
                    "tags": tags,
                    "sosok": sosok,
                    "site": site,
                    "top_margin": top_margin,
                    "bottom_margin": bottom_margin,
                    "margin_settings": margin_settings,
                    "overwrite_decisions": overwrite_decisions,
                    "original_filename": file.filename
                }
                with open(meta_path, "w") as f:
                    json.dump(metadata, f)
                
                # íŒŒì¼ ì €ì¥ ì™„ë£Œ í›„ ì¦‰ì‹œ QUEUED ìƒíƒœë¡œ ë³€ê²½
                task_manager.update_task_status(
                    task_id, 
                    TaskStatus.QUEUED,
                    progress=100,
                    message="ì²˜ë¦¬ ëŒ€ê¸° ì¤‘..."
                )
                
                # ì²˜ë¦¬ íì— ì¶”ê°€ (ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬)
                await task_manager.processing_queue.put(task_id)
                
                # ìƒì„±ëœ ì‘ì—… ì •ë³´ ì €ì¥
                created_task = task_manager.get_task(task_id)
                if created_task:
                    created_tasks.append(created_task)
                
                logging.info(f"âœ… File uploaded and queued: {file.filename} (task_id: {task_id})")
                
            except Exception as e:
                logging.error(f"âŒ Error handling file {file.filename}: {e}")
                # ì‹¤íŒ¨í•œ ì‘ì—…ì€ ì¦‰ì‹œ ì‹¤íŒ¨ ì²˜ë¦¬
                task_manager.fail_task(task_id, str(e))
                
                # ì‹¤íŒ¨í•œ ì‘ì—…ë„ í¬í•¨
                failed_task = task_manager.get_task(task_id)
                if failed_task:
                    created_tasks.append(failed_task)
        
        # ì¦‰ì‹œ ì‘ë‹µ ë°˜í™˜ (ìƒì„±ëœ ì‘ì—… ì •ë³´ í¬í•¨)
        return {
            "status": "accepted",
            "task_ids": task_ids,
            "tasks": created_tasks,  # ìƒì„±ëœ ì‘ì—… ì •ë³´ í¬í•¨
            "message": f"{len(task_ids)}ê°œ íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ. ì²˜ë¦¬ ì¤‘..."
        }
    
    @router.get("/tasks/")
    async def get_tasks(sosok: str, site: str):
        """í˜„ì¥ë³„ ì‘ì—… ëª©ë¡ ì¡°íšŒ"""
        tasks = task_manager.get_tasks_by_site(sosok, site)
        return {"tasks": tasks}
    
    @router.get("/task/{task_id}")
    async def get_task_status(task_id: str):
        """íŠ¹ì • ì‘ì—… ìƒíƒœ ì¡°íšŒ"""
        task = task_manager.get_task(task_id)
        if not task:
            raise HTTPException(status_code=404, detail="Task not found")
        return task
    
    @router.post("/dismiss-task/{task_id}")
    async def dismiss_task(task_id: str):
        """ì‘ì—… UIì—ì„œ ì œê±° (ì™„ë£Œ/ì‹¤íŒ¨ ì‘ì—…)"""
        task = task_manager.get_task(task_id)
        if not task:
            raise HTTPException(status_code=404, detail="Task not found")
        
        # ì‹¤íŒ¨í•œ ì‘ì—…ë„ dismiss ê°€ëŠ¥
        if task['status'] not in ['completed', 'failed']:
            raise HTTPException(status_code=400, detail="Only completed or failed tasks can be dismissed")
        
        task_manager.dismiss_task(task_id)
        
        # ì‹¤íŒ¨í•œ ì‘ì—…ì˜ ì„ì‹œ íŒŒì¼ ì •ë¦¬
        if task['status'] == 'failed':
            try:
                file_path = task_manager.get_task_file_path(task_id)
                if file_path and os.path.exists(file_path):
                    os.remove(file_path)
                meta_path = file_path + ".meta.json"
                if os.path.exists(meta_path):
                    os.remove(meta_path)
            except Exception as e:
                logging.warning(f"âš ï¸ Failed to clean up files for failed task: {e}")
        
        return {"status": "success", "message": "Task dismissed"}
    
    @router.post("/dismiss-completed-tasks")
    async def dismiss_completed_tasks(
        sosok: Optional[str] = Form(None),
        site: Optional[str] = Form(None)
    ):
        """í˜„ì¥ì˜ ëª¨ë“  ì™„ë£Œ/ì‹¤íŒ¨ ì‘ì—… ì œê±°"""
        sosok = sosok.strip() if sosok else ""
        site = site.strip() if site else ""
        
        logging.info(f"ğŸ“¡ Dismissing completed tasks for sosok='{sosok}', site='{site}'")
        
        task_manager.dismiss_completed_tasks(sosok, site)
        
        # ë””ë²„ê¹…ì„ ìœ„í•´ í˜„ì¬ ìƒíƒœ í™•ì¸
        remaining_tasks = task_manager.get_tasks_by_site(sosok, site)
        logging.info(f"ğŸ“Š Remaining tasks after dismiss: {len(remaining_tasks)}")
        
        return {"status": "success", "message": "All completed tasks dismissed"}
    
    @router.post("/cancel-task/{task_id}")
    async def cancel_task(task_id: str):
        """ì§„í–‰ ì¤‘ì¸ ì‘ì—… ì·¨ì†Œ"""
        task = task_manager.get_task(task_id)
        if not task:
            raise HTTPException(status_code=404, detail="Task not found")
        
        # ì‘ì—… ì·¨ì†Œ (task_managerì˜ cancel_task ë©”ì„œë“œ ì‚¬ìš©)
        success = task_manager.cancel_task(task_id)
        
        if success:
            return {"status": "success", "message": "Task cancelled"}
        else:
            raise HTTPException(status_code=400, detail="Cannot cancel this task")
    
    return router


async def process_file_task(task_id: str, document_store, embedder):
    """ì‹¤ì œ íŒŒì¼ ì²˜ë¦¬ ë¡œì§"""
    from util.pdf import split_pdf_by_section_headings, split_pdf_by_token_window
    from util.embedding import embed_document_sections
    from decimal import Decimal
    import pdfplumber
    import re
    import gc
    
    try:
        task = task_manager.get_task(task_id)
        if not task:
            return
        
        # ìƒíƒœë¥¼ ì²˜ë¦¬ ì¤‘ìœ¼ë¡œ ë³€ê²½
        task_manager.update_task_status(task_id, TaskStatus.PROCESSING, 
                                      progress=10, message="íŒŒì¼ ë¶„ì„ ì¤‘...")
        
        # íŒŒì¼ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
        file_path = task_manager.get_task_file_path(task_id)
        
        if not file_path or not os.path.exists(file_path):
            raise Exception("íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        meta_path = file_path + ".meta.json"
        if not os.path.exists(meta_path):
            raise Exception("ë©”íƒ€ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
        with open(meta_path, "r") as f:
            metadata = json.load(f)
        
        ext = os.path.splitext(file_path.lower())[-1]
        sections = []
        total_pages = 0
        
        # íŒŒì¼ ì²˜ë¦¬
        if ext == ".pdf":
            task_manager.update_task_status(task_id, TaskStatus.PROCESSING,
                                          progress=20, message="PDF ë¶„ì„ ì¤‘...")
            
            # ìœ ì§€ë³´ìˆ˜ ë¬¸ì„œ í™•ì¸
            def detect_maintenance_doc(path):
                try:
                    with pdfplumber.open(path) as pdf:
                        for page in pdf.pages[:3]:  # ì²˜ìŒ 3í˜ì´ì§€ë§Œ í™•ì¸
                            text = page.extract_text()
                            if text:
                                first_line = text.strip().splitlines()[0] if text.strip().splitlines() else ""
                                cleaned = re.sub(r"\s+", "", first_line)
                                return cleaned.startswith("ìœ ì§€ë³´ìˆ˜êµë²”")
                except Exception as e:
                    logging.warning(f"âš ï¸ Error detecting maintenance doc: {e}")
                return False
            
            is_maintenance = detect_maintenance_doc(file_path)
            
            # ë§ˆì§„ ì„¤ì •
            margin_map = {}
            if metadata.get("margin_settings"):
                try:
                    margin_map = json.loads(metadata["margin_settings"])
                except:
                    pass
            
            file_margins = margin_map.get(metadata["original_filename"], {})
            top_margin = Decimal(str(file_margins.get("top_margin", metadata.get("top_margin", 0.1))))
            bottom_margin = Decimal(str(file_margins.get("bottom_margin", metadata.get("bottom_margin", 0.1))))
            
            # PDF ë¶„í• 
            try:
                if is_maintenance:
                    sections = split_pdf_by_section_headings(
                        file_path, None, top_margin, bottom_margin,
                        doc_id=None, extract_text_tables=True,
                        auto_detect_header_footer=True,
                        document_title=metadata["original_filename"]
                    )
                else:
                    sections = split_pdf_by_token_window(
                        file_path, top_margin, bottom_margin,
                        700, 100, "./models/KURE-v1",
                        None, True, True
                    )
            except Exception as e:
                logging.error(f"âŒ PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                raise Exception(f"PDF ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            
            # ì´ í˜ì´ì§€ ìˆ˜
            with pdfplumber.open(file_path) as pdf:
                total_pages = len(pdf.pages)
            
            task_manager.update_task_status(task_id, TaskStatus.PROCESSING,
                                          total_pages=total_pages)
        
        elif ext == ".hwpx":
            from util.hwpx import split_hwpx_by_pages
            task_manager.update_task_status(task_id, TaskStatus.PROCESSING,
                                          progress=20, message="HWPX ë¶„ì„ ì¤‘...")
            try:
                sections = split_hwpx_by_pages(file_path, os.path.basename(file_path))
                total_pages = max([s.get("page_number", 0) for s in sections]) if sections else 1
            except Exception as e:
                logging.error(f"âŒ HWPX ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                raise Exception(f"HWPX ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        
        elif ext == ".docx":
            from util.docx import split_docx_by_token_window
            from transformers import AutoTokenizer
            task_manager.update_task_status(task_id, TaskStatus.PROCESSING,
                                          progress=20, message="DOCX ë¶„ì„ ì¤‘...")
            try:
                tokenizer = AutoTokenizer.from_pretrained("./models/KURE-v1")
                sections = split_docx_by_token_window(file_path, 700, 100, tokenizer)
                total_pages = len(sections)
            except Exception as e:
                logging.error(f"âŒ DOCX ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                raise Exception(f"DOCX ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        
        elif ext == ".pptx":
            from util.pptx import split_pptx_by_token_window
            from transformers import AutoTokenizer
            task_manager.update_task_status(task_id, TaskStatus.PROCESSING,
                                          progress=20, message="PPTX ë¶„ì„ ì¤‘...")
            try:
                tokenizer = AutoTokenizer.from_pretrained("./models/KURE-v1")
                sections = split_pptx_by_token_window(file_path, 700, 100, tokenizer)
                total_pages = len(sections)
            except Exception as e:
                logging.error(f"âŒ PPTX ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                raise Exception(f"PPTX ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        
        if not sections:
            raise Exception("ë¬¸ì„œì—ì„œ ì¶”ì¶œëœ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤")
        
        # ì„ë² ë”© ì²˜ë¦¬
        task_manager.update_task_status(task_id, TaskStatus.PROCESSING,
                                      progress=50, message="í…ìŠ¤íŠ¸ ì„ë² ë”© ì¤‘...")
        
        metadata_base = {
            "original_filename": metadata["original_filename"],
            "tags": metadata.get("tags", ""),
            "sosok": metadata["sosok"],
            "site": metadata["site"],
            "file_id": os.path.basename(file_path),
            "total_pdf_pages": total_pages
        }
        
        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ ì½œë°±
        processed_count = 0
        def update_progress():
            nonlocal processed_count
            processed_count += 1
            progress = 50 + int((processed_count / len(sections)) * 40)
            task_manager.update_task_status(
                task_id, TaskStatus.PROCESSING,
                progress=progress,
                processed_pages=processed_count,
                message=f"ì„ë² ë”© ì¤‘... ({processed_count}/{len(sections)})"
            )
        
        # ì„¹ì…˜ë³„ë¡œ ì„ë² ë”© (ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ í¬í•¨)
        embedded_docs = []
        for section in sections:
            if len(section.get("content", "").strip()) < 1:
                continue
            
            meta = {
                **metadata_base,
                "section_title": section.get("title", ""),
                "section_id": section.get("section_id", f"{len(embedded_docs) + 1}"),
                "section_number": len(embedded_docs) + 1,
                "page_number": section.get("page_number", section.get("start_page", 1)),
                "total_pdf_pages": total_pages
            }
            
            from haystack import Document
            content_with_header = f"ë¬¸ì„œ: {metadata['original_filename']}\n<h2>{section.get('title', '')}</h2>\n{section['content']}"
            doc = Document(content=content_with_header, meta=meta)
            
            try:
                embedded_section = embedder.run([doc])["documents"]
                embedded_docs.extend(embedded_section)
            except Exception as e:
                logging.warning(f"âš ï¸ ì„ë² ë”© ì‹¤íŒ¨ (ì„¹ì…˜ {len(embedded_docs) + 1}): {e}")
                continue
            
            update_progress()
            
            # ë©”ëª¨ë¦¬ ê´€ë¦¬
            if len(embedded_docs) % 50 == 0:
                gc.collect()
        
        # ë¬¸ì„œ ì €ì¥
        task_manager.update_task_status(task_id, TaskStatus.PROCESSING,
                                      progress=90, message="ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì¤‘...")
        
        valid_docs = [doc for doc in embedded_docs if doc.embedding is not None and len(doc.embedding) > 0]
        if valid_docs:
            try:
                # ë°°ì¹˜ë¡œ ì €ì¥ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
                batch_size = 100
                for i in range(0, len(valid_docs), batch_size):
                    batch = valid_docs[i:i+batch_size]
                    document_store.write_documents(batch)
                    await asyncio.sleep(0.1)  # ë‹¤ë¥¸ ì‘ì—…ì— CPU ì–‘ë³´
            except Exception as e:
                logging.error(f"âŒ ë¬¸ì„œ ì €ì¥ ì‹¤íŒ¨: {e}")
                raise Exception(f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        else:
            raise Exception("ìœ íš¨í•œ ì„ë² ë”© ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")
        
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        try:
            os.remove(file_path)
            os.remove(meta_path)
        except Exception as e:
            logging.warning(f"âš ï¸ Failed to clean up temporary files: {e}")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del embedded_docs
        gc.collect()
        
        # ì‘ì—… ì™„ë£Œ
        result = {
            "status": "ì„±ê³µ",
            "original_filename": metadata["original_filename"],
            "num_sections": len(valid_docs),
            "total_pages": total_pages
        }
        task_manager.complete_task(task_id, result)
        
    except Exception as e:
        error_msg = str(e)
        logging.error(f"âŒ Error processing task {task_id}: {error_msg}")
        import traceback
        traceback.print_exc()
        
        # ì‘ì—… ì‹¤íŒ¨ ì²˜ë¦¬
        task_manager.fail_task(task_id, error_msg)
        
        # ì‹¤íŒ¨ ì‹œ ì„ì‹œ íŒŒì¼ ì •ë¦¬
        try:
            file_path = task_manager.get_task_file_path(task_id)
            if file_path and os.path.exists(file_path):
                os.remove(file_path)
            meta_path = file_path + ".meta.json"
            if os.path.exists(meta_path):
                os.remove(meta_path)
        except:
            pass
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        gc.collect()