import pdfplumber
import re
from transformers import AutoTokenizer
from decimal import Decimal
import hashlib
from collections import Counter, defaultdict
import os
import uuid
import numpy as np
from datetime import datetime
from .pdf_image_extractor import extract_images_from_pdf, insert_images_in_text
from .pdf_table_extractor import extract_tables_as_text
from .pdf_text_processor import extract_page_content_with_tables


class HeaderFooterDetector:
    """PDF ë¬¸ì„œì˜ ë¨¸ë¦¬ë§/ê¼¬ë¦¬ë§ì„ ìë™ìœ¼ë¡œ íƒì§€í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, pdf_path):
        self.pdf_path = pdf_path
        self.header_regions = {}  # page_num: margin_ratio
        self.footer_regions = {}  # page_num: margin_ratio
    
    def detect_header_footer_regions(self):
        """í…ìŠ¤íŠ¸ íŒ¨í„´ì„ ê¸°ë°˜ìœ¼ë¡œ ë¨¸ë¦¬ë§/ê¼¬ë¦¬ë§ ì˜ì—­ íƒì§€"""
        with pdfplumber.open(self.pdf_path) as pdf:
            total_pages = len(pdf.pages)
            
            # ê° í˜ì´ì§€ì˜ ìƒë‹¨/í•˜ë‹¨ 3ì¤„ì”© ìˆ˜ì§‘
            top_lines = [[], [], []]  # ìƒë‹¨ 1ì¤„, 2ì¤„, 3ì¤„
            bottom_lines = [[], [], []]  # í•˜ë‹¨ 1ì¤„, 2ì¤„, 3ì¤„
            page_infos = []
            
            for page_num, page in enumerate(pdf.pages):
                page_height = float(page.height)
                page_width = float(page.width)
                
                page_data = {
                    "page_num": page_num + 1,
                    "height": page_height,
                    "width": page_width
                }
                
                # í˜ì´ì§€ì˜ ê° ì¤„ì„ Y ìœ„ì¹˜ì™€ í•¨ê»˜ ì¶”ì¶œ
                lines_with_y = self._extract_lines_with_positions(page)
                
                if lines_with_y:
                    # ìƒë‹¨ 3ì¤„ ìˆ˜ì§‘
                    for i in range(min(3, len(lines_with_y))):
                        line_info = lines_with_y[i]
                        top_lines[i].append({
                            "page_num": page_num,
                            "text": line_info["text"],
                            "y_start": line_info["y_start"],
                            "y_end": line_info["y_end"],
                            "avg_y": line_info.get("avg_y", (line_info["y_start"] + line_info["y_end"]) / 2)
                        })
                    
                    # í•˜ë‹¨ 3ì¤„ ìˆ˜ì§‘ (ì—­ìˆœ)
                    for i in range(min(3, len(lines_with_y))):
                        line_info = lines_with_y[-(i+1)]
                        bottom_lines[i].append({
                            "page_num": page_num,
                            "text": line_info["text"],
                            "y_start": line_info["y_start"],
                            "y_end": line_info["y_end"],
                            "avg_y": line_info.get("avg_y", (line_info["y_start"] + line_info["y_end"]) / 2)
                        })
                
                page_infos.append(page_data)
            
            # ê° ì¤„ë³„ë¡œ ë°˜ë³µ íŒ¨í„´ ë¶„ì„
            header_separator_line = self._find_separator_line(top_lines, total_pages, is_header=True)
            footer_separator_line = self._find_separator_line(bottom_lines, total_pages, is_header=False)
            
            # íƒì§€ëœ êµ¬ë¶„ì„ ìœ¼ë¡œ ì˜ì—­ ì„¤ì •
            self._set_regions_from_separator_lines(
                pdf, page_infos, header_separator_line, footer_separator_line
            )
    
    def _extract_lines_with_positions(self, page):
        """í˜ì´ì§€ì—ì„œ ê° ì¤„ì„ Y ìœ„ì¹˜ì™€ í•¨ê»˜ ì¶”ì¶œ - ê°œì„ ëœ ë²„ì „"""
        try:
            chars = page.chars
            if not chars:
                return []
            
            # 1. ë¨¼ì € ëª¨ë“  ë¬¸ìì˜ í‰ê·  ë†’ì´ ê³„ì‚°
            char_heights = [char['height'] for char in chars if 'height' in char]
            avg_char_height = sum(char_heights) / len(char_heights) if char_heights else 12
            
            # 2. ë™ì  ì—¬ìœ ê°’ ì„¤ì • (í‰ê·  ë¬¸ì ë†’ì´ì˜ 20%)
            line_tolerance = avg_char_height * 0.2
            
            # 3. Y ìœ„ì¹˜ë³„ë¡œ ë¬¸ì ê·¸ë£¹í™” (ë” ìœ ì—°í•œ ê·¸ë£¹í™”)
            y_groups = defaultdict(list)
            for char in chars:
                # ê·¼ì²˜ Y ê·¸ë£¹ ì°¾ê¸°
                y_pos = char['top']
                grouped = False
                
                # ê¸°ì¡´ ê·¸ë£¹ë“¤ê³¼ ë¹„êµí•˜ì—¬ ê°€ê¹Œìš´ ê·¸ë£¹ì— ì¶”ê°€
                for existing_y in list(y_groups.keys()):
                    if abs(y_pos - existing_y) <= line_tolerance:
                        y_groups[existing_y].append(char)
                        grouped = True
                        break
                
                # ê°€ê¹Œìš´ ê·¸ë£¹ì´ ì—†ìœ¼ë©´ ìƒˆ ê·¸ë£¹ ìƒì„±
                if not grouped:
                    y_groups[y_pos].append(char)
            
            # 4. ê° ê·¸ë£¹ì„ ì‹¤ì œ Y ìœ„ì¹˜ë¡œ ì •ê·œí™”
            normalized_groups = defaultdict(list)
            for y_key, chars_in_group in y_groups.items():
                if chars_in_group:
                    # ê·¸ë£¹ ë‚´ í‰ê·  Y ìœ„ì¹˜ ê³„ì‚°
                    avg_y = sum(c['top'] for c in chars_in_group) / len(chars_in_group)
                    normalized_groups[avg_y] = chars_in_group
            
            # 5. ê° ì¤„ì„ ì •ë¦¬
            lines = []
            for y_pos in sorted(normalized_groups.keys()):
                line_chars = sorted(normalized_groups[y_pos], key=lambda c: c['x0'])
                
                # ì¤„ í…ìŠ¤íŠ¸ ìƒì„±
                line_text = ""
                last_x1 = None
                for char in line_chars:
                    if last_x1 is not None and char['x0'] - last_x1 > char['width'] * 0.3:
                        line_text += " "
                    line_text += char['text']
                    last_x1 = char['x1']
                
                if line_text.strip():
                    lines.append({
                        "text": line_text.strip(),
                        "y_start": min(c['top'] for c in line_chars),
                        "y_end": max(c['bottom'] for c in line_chars),
                        "avg_y": y_pos,
                        "char_count": len(line_chars)
                    })
            
            return lines
            
        except Exception as e:
            print(f"Error extracting lines: {e}")
            return []
    
    def _find_separator_line(self, lines_by_position, total_pages, is_header=True):
        """ê° ì¤„ë³„ë¡œ ë°˜ë³µ íŒ¨í„´ì„ ì°¾ì•„ êµ¬ë¶„ì„  íƒì§€ - ê°œì„ ëœ ë²„ì „"""
        threshold = 0.4  # 40%ë¡œ ë‚®ì¶¤ (ê¸°ì¡´ 50%)
        separator_candidates = []
        
        # ê° ì¤„ ìœ„ì¹˜ë³„ë¡œ ë¶„ì„
        for line_num, line_list in enumerate(lines_by_position):
            if not line_list:
                continue
            
            # í…ìŠ¤íŠ¸ ìœ ì‚¬ì„± ê·¸ë£¹í™” (ì™„ì „ ì¼ì¹˜ ëŒ€ì‹  ìœ ì‚¬ì„± ê¸°ë°˜)
            text_groups = defaultdict(list)
            
            for item in line_list:
                text = item["text"]
                matched = False
                
                # ê¸°ì¡´ ê·¸ë£¹ê³¼ ìœ ì‚¬ì„± ë¹„êµ
                for group_text in list(text_groups.keys()):
                    similarity = self._calculate_text_similarity(text, group_text)
                    if similarity > 0.85:  # 85% ì´ìƒ ìœ ì‚¬í•˜ë©´ ê°™ì€ ê·¸ë£¹
                        text_groups[group_text].append(item)
                        matched = True
                        break
                
                if not matched:
                    text_groups[text].append(item)
            
            # ê°€ì¥ ë§ì´ ë°˜ë³µë˜ëŠ” í…ìŠ¤íŠ¸ ê·¸ë£¹ ì°¾ê¸°
            for group_text, items in text_groups.items():
                percentage = len(items) / total_pages
                if percentage >= threshold:
                    # Y ìœ„ì¹˜ ë³€ë™ì„± ê³„ì‚°
                    y_positions = []
                    for item in items:
                        y_positions.append({
                            "page_num": item["page_num"],
                            "y_start": item["y_start"],
                            "y_end": item["y_end"],
                            "avg_y": item.get("avg_y", (item["y_start"] + item["y_end"]) / 2)
                        })
                    
                    # Y ìœ„ì¹˜ í‘œì¤€í¸ì°¨ ê³„ì‚°
                    avg_ys = [pos["avg_y"] for pos in y_positions]
                    if len(avg_ys) > 1:
                        mean_y = sum(avg_ys) / len(avg_ys)
                        variance = sum((y - mean_y) ** 2 for y in avg_ys) / len(avg_ys)
                        std_dev = variance ** 0.5
                        
                        # í‘œì¤€í¸ì°¨ê°€ ì‘ìœ¼ë©´ (ì¼ê´€ëœ ìœ„ì¹˜) ë” ë†’ì€ ì ìˆ˜
                        consistency_score = 1 / (1 + std_dev)
                    else:
                        consistency_score = 1.0
                    
                    separator_candidates.append({
                        "line_num": line_num + 1,
                        "text": group_text,
                        "count": len(items),
                        "percentage": percentage * 100,
                        "y_positions": y_positions,
                        "consistency_score": consistency_score,
                        "total_score": percentage * consistency_score
                    })
        
        # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ êµ¬ë¶„ì„  ì„ íƒ
        if separator_candidates:
            # total_score ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            separator_candidates.sort(key=lambda x: x["total_score"], reverse=True)
            best_candidate = separator_candidates[0]
            
            # ë””ë²„ê·¸ ì¶œë ¥
            print(f"{'Header' if is_header else 'Footer'} separator candidates:")
            for cand in separator_candidates[:3]:  # ìƒìœ„ 3ê°œë§Œ ì¶œë ¥
                print(f"  Line {cand['line_num']}: '{cand['text'][:50]}...' "
                      f"({cand['percentage']:.1f}%, consistency: {cand['consistency_score']:.2f})")
            
            return best_candidate
        
        return None
    
    def _calculate_text_similarity(self, text1, text2):
        """ë‘ í…ìŠ¤íŠ¸ì˜ ìœ ì‚¬ë„ ê³„ì‚° (0~1)"""
        # ê°„ë‹¨í•œ ë¬¸ì ê¸°ë°˜ ìœ ì‚¬ë„ (ë” ì •êµí•œ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ëŒ€ì²´ ê°€ëŠ¥)
        if not text1 or not text2:
            return 0.0
        
        # ì •ê·œí™”
        text1 = text1.strip().lower()
        text2 = text2.strip().lower()
        
        if text1 == text2:
            return 1.0
        
        # ë ˆë²¤ìŠˆíƒ€ì¸ ê±°ë¦¬ ê¸°ë°˜ ìœ ì‚¬ë„
        distance = self._levenshtein_distance(text1, text2)
        max_len = max(len(text1), len(text2))
        
        if max_len == 0:
            return 1.0
        
        similarity = 1 - (distance / max_len)
        return max(0.0, similarity)
    
    def _levenshtein_distance(self, s1, s2):
        """ë ˆë²¤ìŠˆíƒ€ì¸ ê±°ë¦¬ ê³„ì‚°"""
        if len(s1) < len(s2):
            return self._levenshtein_distance(s2, s1)
        
        if len(s2) == 0:
            return len(s1)
        
        previous_row = range(len(s2) + 1)
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row
        
        return previous_row[-1]
    
    def _set_regions_from_separator_lines(self, pdf, page_infos, header_separator, footer_separator):
        """íƒì§€ëœ êµ¬ë¶„ì„ ì„ ê¸°ì¤€ìœ¼ë¡œ ì œê±°í•  ì˜ì—­ ì„¤ì • - ê°œì„ ëœ ë²„ì „"""
        # ë¨¸ë¦¬ë§ ì˜ì—­ ì„¤ì •
        if header_separator:
            for y_info in header_separator["y_positions"]:
                page_num = y_info["page_num"]
                if page_num < len(pdf.pages):
                    page_height = float(pdf.pages[page_num].height)
                    
                    # ë™ì  ì—¬ìœ ê°’ ê³„ì‚° (í˜ì´ì§€ ë†’ì´ì˜ 1% ë˜ëŠ” 10í¬ì¸íŠ¸ ì¤‘ í° ê°’)
                    dynamic_margin = max(page_height * 0.01, 10)
                    
                    # êµ¬ë¶„ì„  í¬í•¨í•˜ì—¬ ë°”ê¹¥ìª½ ì œê±°
                    cut_y_from_top = y_info["y_end"] + dynamic_margin
                    margin_ratio = cut_y_from_top / page_height
                    
                    # ìµœëŒ€ 30% ì œí•œ
                    self.header_regions[page_num] = min(margin_ratio, 0.3)
                    
                    if page_num < len(page_infos):
                        page_infos[page_num]["header_y_end"] = y_info["y_end"]
                        page_infos[page_num]["header_cut_y"] = cut_y_from_top
                        page_infos[page_num]["header_ratio"] = self.header_regions[page_num]
                        page_infos[page_num]["header_line_num"] = header_separator["line_num"]
                        page_infos[page_num]["header_margin"] = dynamic_margin
        
        # ê¼¬ë¦¬ë§ ì˜ì—­ ì„¤ì •
        if footer_separator:
            for y_info in footer_separator["y_positions"]:
                page_num = y_info["page_num"]
                if page_num < len(pdf.pages):
                    page_height = float(pdf.pages[page_num].height)
                    
                    # ë™ì  ì—¬ìœ ê°’ ê³„ì‚°
                    dynamic_margin = max(page_height * 0.01, 10)
                    
                    # êµ¬ë¶„ì„  í¬í•¨í•˜ì—¬ ë°”ê¹¥ìª½ ì œê±°
                    cut_y_from_top = y_info["y_start"] - dynamic_margin
                    margin_ratio = (page_height - cut_y_from_top) / page_height
                    
                    # ìµœëŒ€ 30% ì œí•œ
                    self.footer_regions[page_num] = min(margin_ratio, 0.3)
                    
                    if page_num < len(page_infos):
                        page_infos[page_num]["footer_y_start"] = y_info["y_start"]
                        page_infos[page_num]["footer_cut_y"] = cut_y_from_top
                        page_infos[page_num]["footer_ratio"] = self.footer_regions[page_num]
                        page_infos[page_num]["footer_line_num"] = footer_separator["line_num"]
                        page_infos[page_num]["footer_margin"] = dynamic_margin
    
    def _find_separated_texts(self, page):
        """í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ê³¼ ë¶„ë¦¬ëœ ë¨¸ë¦¬ë§/ê¼¬ë¦¬ë§ í…ìŠ¤íŠ¸ ì°¾ê¸°"""
        result = {"header": None, "footer": None}
        
        try:
            chars = page.chars
            if not chars:
                return result
            
            page_height = float(page.height)
            
            # Y ìœ„ì¹˜ë³„ë¡œ ë¬¸ì ê·¸ë£¹í™” (pdfplumber ì¢Œí‘œê³„: ìœ„ì—ì„œë¶€í„°)
            y_groups = defaultdict(list)
            for char in chars:
                # pdfplumberì—ì„œ char['top']ì€ ì´ë¯¸ ìœ„ì—ì„œë¶€í„°ì˜ ê±°ë¦¬
                y_key = round(char['top'], 3)
                y_groups[y_key].append(char)
            
            # Y ìœ„ì¹˜ ì •ë ¬ (ìœ„ì—ì„œ ì•„ë˜ë¡œ)
            sorted_y_positions = sorted(y_groups.keys())
            if not sorted_y_positions:
                return result
            
            # í…ìŠ¤íŠ¸ ë¸”ë¡ ì°¾ê¸° (ì—°ì†ëœ Y ìœ„ì¹˜ë¥¼ í•˜ë‚˜ì˜ ë¸”ë¡ìœ¼ë¡œ)
            text_blocks = []
            current_block = []
            last_y = None
            
            for y_pos in sorted_y_positions:
                if last_y is None or y_pos - last_y < 15:  # 15í¬ì¸íŠ¸ ì´ë‚´ë©´ ê°™ì€ ë¸”ë¡
                    current_block.append(y_pos)
                else:
                    if current_block:
                        text_blocks.append(current_block)
                    current_block = [y_pos]
                last_y = y_pos
            
            if current_block:
                text_blocks.append(current_block)
            
            # ë¨¸ë¦¬ë§ ì°¾ê¸°: ì²« ë²ˆì§¸ ë¸”ë¡ì´ ì¶©ë¶„íˆ ë¶„ë¦¬ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            if len(text_blocks) >= 2:
                first_block = text_blocks[0]
                second_block = text_blocks[1]
                
                # ì²« ë¸”ë¡ì˜ ëê³¼ ë‘ ë²ˆì§¸ ë¸”ë¡ì˜ ì‹œì‘ ì‚¬ì´ ê°„ê²©
                gap = min(second_block) - max(first_block)
                
                if gap > 30:  # 30í¬ì¸íŠ¸ ì´ìƒ ë–¨ì–´ì ¸ ìˆìœ¼ë©´ ë¨¸ë¦¬ë§ë¡œ ê°„ì£¼
                    # ì²« ë¸”ë¡ì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    header_chars = []
                    for y in first_block:
                        header_chars.extend(y_groups[y])
                    
                    header_chars.sort(key=lambda c: (c['top'], c['x0']))
                    header_text = self._chars_to_text(header_chars)
                    
                    if header_text.strip():
                        result["header"] = {
                            "text": header_text.strip(),
                            "y_start": min(c['top'] for c in header_chars),  # ìœ„ì—ì„œë¶€í„°
                            "y_end": max(c['bottom'] for c in header_chars)  # ìœ„ì—ì„œë¶€í„°
                        }
            
            # ê¼¬ë¦¬ë§ ì°¾ê¸°: ë§ˆì§€ë§‰ ë¸”ë¡ì´ ì¶©ë¶„íˆ ë¶„ë¦¬ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            if len(text_blocks) >= 2:
                last_block = text_blocks[-1]
                second_last_block = text_blocks[-2]
                
                # ë§ˆì§€ë§‰ ë¸”ë¡ê³¼ ê·¸ ì´ì „ ë¸”ë¡ ì‚¬ì´ ê°„ê²©
                gap = min(last_block) - max(second_last_block)
                
                if gap > 30:  # 30í¬ì¸íŠ¸ ì´ìƒ ë–¨ì–´ì ¸ ìˆìœ¼ë©´ ê¼¬ë¦¬ë§ë¡œ ê°„ì£¼
                    # ë§ˆì§€ë§‰ ë¸”ë¡ì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    footer_chars = []
                    for y in last_block:
                        footer_chars.extend(y_groups[y])
                    
                    footer_chars.sort(key=lambda c: (c['top'], c['x0']))
                    footer_text = self._chars_to_text(footer_chars)
                    
                    if footer_text.strip():
                        result["footer"] = {
                            "text": footer_text.strip(),
                            "y_start": min(c['top'] for c in footer_chars),  # ìœ„ì—ì„œë¶€í„°
                            "y_end": max(c['bottom'] for c in footer_chars)  # ìœ„ì—ì„œë¶€í„°
                        }
            
            return result
            
        except Exception as e:
            return result
    
    def _chars_to_text(self, chars):
        """ë¬¸ì ë¦¬ìŠ¤íŠ¸ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        if not chars:
            return ""
        
        # Y ìœ„ì¹˜ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ì¤„ ë‹¨ìœ„ë¡œ ì²˜ë¦¬ (ë” ì •ë°€í•œ ì†Œìˆ˜ì )
        lines = defaultdict(list)
        for char in chars:
            y_key = round(char['top'], 3)  # ì†Œìˆ˜ì  3ìë¦¬
            lines[y_key].append(char)
        
        # ê° ì¤„ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        text_lines = []
        for y_pos in sorted(lines.keys()):
            line_chars = sorted(lines[y_pos], key=lambda c: c['x0'])
            line_text = ""
            last_x1 = None
            
            for char in line_chars:
                # ë‹¨ì–´ ì‚¬ì´ ê³µë°± ì¶”ê°€
                if last_x1 is not None and char['x0'] - last_x1 > char['width'] * 0.3:
                    line_text += " "
                line_text += char['text']
                last_x1 = char['x1']
            
            text_lines.append(line_text)
        
        return "\n".join(text_lines)
    
    def get_margin_ratios(self):
        """íƒì§€ëœ ì˜ì—­ì„ margin ratioë¡œ ë³€í™˜"""
        self.detect_header_footer_regions()
        
        # í‰ê· ê°’ ê³„ì‚° (ë” ì •ë°€í•œ ì†Œìˆ˜ì )
        avg_top_margin = 0.0
        avg_bottom_margin = 0.0
        
        if self.header_regions:
            # numpyë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  ì •ë°€í•œ í‰ê·  ê³„ì‚°
            total = sum(self.header_regions.values())
            avg_top_margin = total / len(self.header_regions)
        
        if self.footer_regions:
            total = sum(self.footer_regions.values())
            avg_bottom_margin = total / len(self.footer_regions)
        
        return avg_top_margin, avg_bottom_margin


def auto_detect_margins(pdf_path):
    """PDFì˜ ë¨¸ë¦¬ë§/ê¼¬ë¦¬ë§ ì˜ì—­ì„ ìë™ìœ¼ë¡œ íƒì§€í•˜ì—¬ margin ratio ë°˜í™˜"""
    detector = HeaderFooterDetector(pdf_path)
    top_margin, bottom_margin = detector.get_margin_ratios()
    
    print(f"ğŸ” ìë™ íƒì§€ëœ margin - ìƒë‹¨: {top_margin*100:.3f}%, í•˜ë‹¨: {bottom_margin*100:.3f}%")
    
    return top_margin, bottom_margin


def clean_text_by_fixed_margins_with_tables(pdf_path, top_margin_ratio=0.15, bottom_margin_ratio=0.1, 
                                           image_map=None, extract_tables_as_text_flag=True,
                                           auto_detect_header_footer=True):
    """
    Clean text by removing margins and optionally insert image tags and text tables.
    Enhanced to extract text and tables separately, then combine them in order.
    Now with automatic header/footer detection option.
    """
    # ìë™ ë¨¸ë¦¬ë§/ê¼¬ë¦¬ë§ íƒì§€
    if auto_detect_header_footer:
        detected_top, detected_bottom = auto_detect_margins(pdf_path)
        # ìë™ íƒì§€ëœ ê°’ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
        if detected_top > 0:
            top_margin_ratio = detected_top
        if detected_bottom > 0:
            bottom_margin_ratio = detected_bottom
    
    top_margin_ratio = float(top_margin_ratio)
    bottom_margin_ratio = float(bottom_margin_ratio)
    
    cleaned_pages = []
    
    with pdfplumber.open(pdf_path) as pdf:
        for page_num, page in enumerate(pdf.pages, start=1):
            page_height = float(page.height)
            
            # PDF ì¢Œí‘œê³„: bottomì€ ì•„ë˜ì—ì„œë¶€í„°, topì€ ìœ„ì—ì„œë¶€í„°
            # bottom_margin_ratioê°€ 0.1ì´ë©´ ì•„ë˜ 10%ë¥¼ ì œê±°
            # top_margin_ratioê°€ 0.1ì´ë©´ ìœ„ 10%ë¥¼ ì œê±°
            
            # pdfplumberì˜ bboxëŠ” (x0, y0, x1, y1) í˜•ì‹
            # y0ëŠ” ìœ„ì—ì„œë¶€í„°ì˜ ê±°ë¦¬, y1ì€ ì•„ë˜ê¹Œì§€ì˜ ê±°ë¦¬
            y0_from_top = page_height * top_margin_ratio      # ìœ„ì—ì„œë¶€í„° ì œê±°í•  ê±°ë¦¬
            y1_from_top = page_height * (1 - bottom_margin_ratio)  # ìœ„ì—ì„œë¶€í„° ë³´ì¡´í•  ë ê±°ë¦¬

            try:
                # pdfplumberì˜ within_bboxëŠ” ìœ„ì—ì„œë¶€í„°ì˜ ì¢Œí‘œë¥¼ ì‚¬ìš©
                cropped = page.within_bbox((0, y0_from_top, float(page.width), y1_from_top))
                
                if extract_tables_as_text_flag:
                    # Extract text with table regions marked
                    page_content = extract_page_content_with_tables(cropped, page_num)
                else:
                    # Original text extraction
                    page_content = cropped.extract_text(x_tolerance=1, y_tolerance=1) or ""
                
                if not page_content or len(page_content.strip()) == 0:
                    print(f"âš ï¸ Fallback on page {page_num}: cropped text is empty")
                    page_content = "[CROPPING FAILED]"
                    
            except Exception as e:
                print(f"âŒ Exception on page {page_num}: {e}")
                page_content = page.extract_text() or ""

            # Insert images if available
            if image_map and page_num in image_map:
                page_content = insert_images_in_text(page_content.strip(), page_num, image_map)
            
            cleaned_pages.append(page_content.strip())
    
    return cleaned_pages


def clean_text_by_fixed_margins(pdf_path, top_margin_ratio=0.15, bottom_margin_ratio=0.1, 
                               image_map=None, auto_detect_header_footer=True):
    """
    Original clean text function for backward compatibility.
    """
    return clean_text_by_fixed_margins_with_tables(
        pdf_path, top_margin_ratio, bottom_margin_ratio, 
        image_map, extract_tables_as_text_flag=False,
        auto_detect_header_footer=auto_detect_header_footer
    )


def split_pdf_by_token_window(pdf_path, top_margin_ratio=0, bottom_margin_ratio=0,
                              window_size=700, overlap=100,
                              model_name="./models/KURE-v1", doc_id=None,
                              extract_text_tables=True, auto_detect_header_footer=True):
    """
    Splits PDF content into token-based chunks using the BGE/KURE tokenizer.
    Includes estimated start_page for each chunk, extracts images, and text tables.
    Now with automatic header/footer detection option.
    """
    if doc_id is None:
        doc_id = str(uuid.uuid4())
    
    # Extract images first
    image_map = extract_images_from_pdf(pdf_path, doc_id)
    
    # Use modified function that includes text tables
    page_texts = clean_text_by_fixed_margins_with_tables(
        pdf_path, top_margin_ratio, bottom_margin_ratio, 
        image_map, extract_text_tables,
        auto_detect_header_footer=auto_detect_header_footer
    )
    
    tokenizer = AutoTokenizer.from_pretrained(model_name, local_files_only=True)

    # Encode each page separately to map tokens to pages
    page_token_boundaries = []
    all_tokens = []
    for i, text in enumerate(page_texts):
        tokens = tokenizer.encode(text, add_special_tokens=False)
        page_token_boundaries.append((len(all_tokens), len(all_tokens) + len(tokens), i + 1))  # (start_idx, end_idx, page_number)
        all_tokens.extend(tokens)

    chunks = []
    start = 0
    idx = 1
    total_tokens = len(all_tokens)

    while start < total_tokens:
        end = min(start + window_size, total_tokens)
        token_chunk = all_tokens[start:end]
        text_chunk = tokenizer.decode(token_chunk, skip_special_tokens=True)

        # Estimate start_page by finding the first page whose token span overlaps this chunk
        start_page = next((page_num for s_idx, e_idx, page_num in page_token_boundaries if s_idx <= start < e_idx), 1)

        chunks.append({
            "title": f"Chunk {idx}",
            "content": text_chunk,
            "start_token": start,
            "start_page": start_page
        })

        idx += 1
        start += window_size - overlap

    return chunks


def split_pdf_by_pages(pdf_path, top_margin_ratio=0, bottom_margin_ratio=0, 
                       doc_id=None, extract_text_tables=True, auto_detect_header_footer=True):
    """
    Split PDF by pages with image extraction and text tables.
    Now with automatic header/footer detection option.
    """
    if doc_id is None:
        doc_id = str(uuid.uuid4())
    
    # Extract images first
    image_map = extract_images_from_pdf(pdf_path, doc_id)
    
    # Use modified function that includes text tables
    page_texts = clean_text_by_fixed_margins_with_tables(
        pdf_path, top_margin_ratio, bottom_margin_ratio, 
        image_map, extract_text_tables,
        auto_detect_header_footer=auto_detect_header_footer
    )
    
    pages = []
    for i, text in enumerate(page_texts, start=1):
        pages.append({
            "content": text.strip(),
            "page_number": i
        })
    return pages


def split_pdf_by_section_headings(pdf_path, pattern=None, top_margin_ratio=0.1, 
                                  bottom_margin_ratio=0.1, doc_id=None,
                                  extract_text_tables=True, auto_detect_header_footer=True,
                                  document_title=None):
    """
    Split PDF by section headings with image extraction and text tables.
    Now with automatic header/footer detection option.
    """
    if doc_id is None:
        doc_id = str(uuid.uuid4())
    
    # Extract images first
    image_map = extract_images_from_pdf(pdf_path, doc_id)

    ############################################
    # 1) Heuristic function to merge lines
    ############################################
    def heuristic_join_lines(lines):
        """
        Merges wrapped lines intelligently, preserving real paragraph breaks
        (blank lines or bullet points), but avoiding excessive newlines.
        Preserves table structure by not merging lines within tables.
        """

        bullet_or_heading_pattern = (
            r'^\s*('
            r'[-*â€¢â¦â—âˆ™Â·â€£â–ªâ–ºâ˜ãƒ»]'            # Bullet symbols including ãƒ»
            r'|[0-9]+\.'                  # Numbered list: 1. 2.
            r'|\([0-9]+\)'               # (1), (2), ...
            r'|\[[^\]]+\]'              # [í…ìŠ¤íŠ¸], [í™•ì¸], etc.
            r')\s*'
        )

        result = []
        current_paragraph = []
        in_table = False

        for raw_line in lines:
            line = raw_line.strip()

            # Check if we're entering a table
            if re.match(r'^\[í‘œ\s+\d+\s+-\s+í˜ì´ì§€\s+\d+\s+ì‹œì‘\]$', line):
                # Flush current paragraph
                if current_paragraph:
                    result.append(" ".join(current_paragraph))
                    current_paragraph = []
                in_table = True
                result.append(raw_line)  # Keep table start marker
                continue
            
            # Check if we're exiting a table
            if re.match(r'^\[í‘œ\s+\d+\s+-\s+í˜ì´ì§€\s+\d+\s+ë\]$', line):
                in_table = False
                result.append(raw_line)  # Keep table end marker
                continue
            
            # If we're in a table, preserve all formatting
            if in_table:
                result.append(raw_line)  # Keep original line with all formatting
                continue

            # Regular text processing (outside of tables)
            if any(b in line for b in ['â¦', 'â€¢', '-', '*']):
                print(f"ğŸ” Checking for bullet: {repr(line)}")

            # âœ… Match bullet pattern exactly
            if re.match(bullet_or_heading_pattern, line):
                print(f"âœ… Bullet matched: {repr(line)}")  # debug success
                if current_paragraph:
                    result.append(" ".join(current_paragraph))
                    current_paragraph = []
                result.append(line)
            elif not line:
                # Blank line = new paragraph
                if current_paragraph:
                    result.append(" ".join(current_paragraph))
                    current_paragraph = []
                result.append("")
            else:
                current_paragraph.append(line)

        # Flush any remaining content
        if current_paragraph:
            result.append(" ".join(current_paragraph))

        # Join all lines
        return "\n".join(result)


    ############################################
    # 2) Default pattern: Updated to include ì œ N ì¥ with flexible spaces
    ############################################
    if pattern is None:
        # Matches "ì œ N ì¥", "ì²¨ë¶€ N" with flexible spaces, and numeric headings like "1.2.3 Some text"
        pattern = (
            r'(?m)^'
            r'(?:ì œ\s*\d{1,3}\s*ì¥)'  # "ì œ N ì¥" with flexible spaces
            r'|'
            r'(?:ì²¨ë¶€\s*\d{1,3})'     # "ì²¨ë¶€ N" with flexible spaces
            r'|'
            r'(?:\d{1,2}(?:\.\d{1,2}){1,3})'
            r'(?:\.?)\s+'
            r'(?!Radio Navigation and Landing Aids)'
            r'[^\s\d].+'
        )

    ############################################
    # 3) Extract lines from PDF (with margins, images, and text tables)
    ############################################
    page_texts = clean_text_by_fixed_margins_with_tables(
        pdf_path, top_margin_ratio, bottom_margin_ratio, 
        image_map, extract_text_tables,
        auto_detect_header_footer=auto_detect_header_footer
    )

    # Remove table content from section matching
    def remove_table_content(text):
        """Remove content between table markers"""
        # Pattern to match everything between table start and end markers
        table_pattern = r'\[í‘œ\s+\d+\s+-\s+í˜ì´ì§€\s+\d+\s+ì‹œì‘\].*?\[í‘œ\s+\d+\s+-\s+í˜ì´ì§€\s+\d+\s+ë\]'
        # Remove table content but keep the rest
        cleaned_text = re.sub(table_pattern, '', text, flags=re.DOTALL)
        return cleaned_text

    # Collect all matches excluding table content
    matches = []
    korean_chapter_count = 0  # Track count of "ì œ N ì¥" matches
    
    for page_idx, page_text in enumerate(page_texts):
        # Remove table content before matching
        cleaned_page_text = remove_table_content(page_text)
        
        # Use re.MULTILINE so ^ matches line starts within text
        for match in re.finditer(pattern, cleaned_page_text, flags=re.MULTILINE):
            # Check if this is a Korean chapter
            if re.match(r'ì œ\s*\d{1,3}\s*ì¥', match.group().strip()):
                korean_chapter_count += 1
                # Skip the first occurrence of "ì œ 1 ì¥"
                if korean_chapter_count == 1 and re.match(r'ì œ\s*1\s*ì¥', match.group().strip()):
                    print(f"â­ï¸ Skipping first occurrence of ì œ 1 ì¥ on page {page_idx + 1}")
                    continue
            
            # Find the actual position in the original text (with tables)
            match_text = match.group()
            actual_position = page_text.find(match_text)
            if actual_position != -1:
                # Create a new match object with the correct position
                matches.append({
                    "match": match,
                    "page_idx": page_idx,
                    "text": page_text,  # Use original text with tables
                    "actual_start": actual_position,
                    "match_text": match_text
                })

    # Sort matches by their absolute position
    matches.sort(key=lambda m: (m["page_idx"], m["actual_start"]))
    
    # Debug: Print all found matches
    print(f"\nğŸ“‹ Total matches found: {len(matches)}")
    for idx, m in enumerate(matches):
        print(f"  {idx}: {m['match_text']} (page {m['page_idx'] + 1})")

    # Count occurrences of each section title
    all_section_titles = []
    for item in matches:
        section_raw = re.sub(r'\s+', ' ', item["match_text"].strip())
        sec_match = re.match(r'(\d{1,2}(?:\.\d{1,2}){0,3})', section_raw)
        if sec_match:
            all_section_titles.append(sec_match.group(1))
        else:
            # For Korean chapters
            kor_match = re.match(r'ì œ\s*(\d{1,3})\s*ì¥', section_raw)
            if kor_match:
                all_section_titles.append(f"ì œ {kor_match.group(1)} ì¥")
            else:
                # For appendices
                appendix_match = re.match(r'ì²¨ë¶€\s*(\d{1,3})', section_raw)
                if appendix_match:
                    all_section_titles.append(f"ì²¨ë¶€ {appendix_match.group(1)}")

    section_title_counts = Counter(all_section_titles)

    ############################################
    # 4) Prepare for section building
    ############################################
    sections = []
    section_hierarchy = {}
    seen_section_numbers = set()
    collect = False
    last_valid_number = None
    merged_out_of_order = set()
    merged_content_cache = set()
    expecting_next_subsection = False  # Flag to track if we're expecting N.1 after ì œ N ì¥
    expected_chapter_num = None  # Track which chapter number we're expecting subsections for
    in_appendix_mode = False  # Track if we've entered appendix mode
    last_appendix_num = 0  # Track last appendix number
    current_chapter_sections = set()  # Track all sections in current chapter

    def parse_number(n):
        return [int(p) for p in n.split(".") if p.isdigit()]

    def is_strictly_next(prev, current, current_chapter=None, current_chapter_sections=None):
        """Check if current section number follows the expected sequence"""
        # If we just processed a Korean chapter, any subsection of that chapter is valid
        if current_chapter is not None and len(current) >= 1 and current[0] == current_chapter:
            # Check if it's a valid subsection that we haven't seen before
            current_str = '.'.join(map(str, current))
            if current_str in current_chapter_sections:
                print(f"âš ï¸ Section {current_str} already exists in chapter {current_chapter}")
                return False
            return True
        
        if not prev:
            return True  # Accept any valid section number when starting

        # Case: same depth â†’ must increment last digit
        if len(current) == len(prev):
            return current[:-1] == prev[:-1] and current[-1] == prev[-1] + 1

        # Case: one level deeper â†’ must start with 1 (child section)
        if len(current) == len(prev) + 1:
            return current[:-1] == prev and current[-1] == 1

        # Case: back to higher level
        for i in range(min(len(prev), len(current))):
            if current[i] == prev[i]:
                continue
            if current[i] == prev[i] + 1:
                return all(c == 1 for c in current[i + 1:])
            return False

        return False

    def is_out_of_order(current, current_chapter=None, current_chapter_sections=None):
        """Check if the current section is out of order within the current chapter"""
        if current_chapter is None or len(current) < 1:
            return False
        
        # Must be in the current chapter
        if current[0] != current_chapter:
            return True
        
        # For sections like 5.20, if we see 5.1 or 5, it's out of order
        current_str = '.'.join(map(str, current))
        
        # Check all existing sections in current chapter
        for existing in current_chapter_sections:
            existing_parts = parse_number(existing)
            
            # If current has fewer parts than existing, check if it's a parent that should have come earlier
            if len(current) < len(existing_parts):
                # Check if current is a parent of existing
                if existing_parts[:len(current)] == current:
                    print(f"âš ï¸ Section {current_str} is out of order - should come before {existing}")
                    return True
            
            # If same depth, check if current should have come earlier
            elif len(current) == len(existing_parts):
                # Compare at same level
                if current[:-1] == existing_parts[:-1] and current[-1] < existing_parts[-1]:
                    print(f"âš ï¸ Section {current_str} is out of order - should come before {existing}")
                    return True
        
        return False

    def merge_out_of_order_section(section_title_raw, section_content, section_number):
        """Merge out-of-order section into previous section"""
        content_hash = hash_text(section_content)
        if sections and content_hash not in merged_content_cache:
            print(f"ğŸ“ Merging out-of-order section {section_number} into previous")
            sections[-1]["content"] += "\n\n" + section_title_raw + "\n" + section_content
            merged_content_cache.add(content_hash)
            merged_out_of_order.add(section_number)
            return True
        else:
            print(f"â­ï¸ Duplicate out-of-order content skipped: {section_title_raw}")
            return False

    def hash_text(text):
        """Hash section content to detect duplicates reliably."""
        return hashlib.sha256(text.strip().encode("utf-8")).hexdigest()

    ############################################
    # 5) Build sections from matches
    ############################################
    for idx, item in enumerate(matches):
        match = item["match"]
        page_idx = item["page_idx"]
        page_text = item["text"]
        actual_start = item["actual_start"]

        # The raw heading text
        section_title_raw = re.sub(r'\s+', ' ', match.group().strip())

        # Parse lines from current heading to next
        # Use actual_start instead of match.start() for correct positioning
        lines = []

        # Next heading info
        next_page_idx = matches[idx + 1]["page_idx"] if idx + 1 < len(matches) else len(page_texts)
        next_actual_start = matches[idx + 1]["actual_start"] if idx + 1 < len(matches) else None
        next_page_text = page_texts[next_page_idx] if next_page_idx < len(page_texts) else ""

        # Calculate the end position of the current match
        current_match_end = actual_start + len(item["match_text"])

        # Handle content range between this heading and the next heading
        if next_page_idx == page_idx and next_actual_start is not None:
            # Both headings are on the same page â€” slice between them
            lines = page_text[current_match_end:next_actual_start].splitlines()
        else:
            # 1. From heading to end of current page
            lines.extend(page_text[current_match_end:].splitlines())

            # 2. Full in-between pages
            for i in range(page_idx + 1, next_page_idx):
                lines.extend(page_texts[i].splitlines())

            # 3. Start of next page up to next heading
            if next_actual_start is not None and next_page_idx < len(page_texts):
                lines.extend(next_page_text[:next_actual_start].splitlines())

        # Convert lines â†’ final content
        section_content = heuristic_join_lines(lines)

        # Check if heading is Korean "ì œ N ì¥"
        kor_match = re.match(r'ì œ\s*(\d{1,3})\s*ì¥', section_title_raw)
        if kor_match:
            is_korean_chapter = True
            is_appendix = False
            chapter_num = int(kor_match.group(1))
            section_number = f"ì œ {chapter_num} ì¥"  # Normalize format
            section_number_list = [chapter_num]  # Use chapter number for hierarchy
            
            # If this is ì œ 1 ì¥, start collecting
            if chapter_num == 1:
                collect = True
                print(f"âœ… Starting collection with {section_number}")
            
            # For any ì œ N ì¥, expect N.1 next
            if collect:
                expecting_next_subsection = True
                expected_chapter_num = chapter_num
                last_valid_number = None  # Reset for new chapter sequence
                in_appendix_mode = False  # Reset appendix mode
                current_chapter_sections = set()  # Reset chapter sections
                print(f"âœ… Found {section_number}, expecting {chapter_num}.1 next")
        else:
            # Check if heading is "ì²¨ë¶€ N"
            appendix_match = re.match(r'ì²¨ë¶€\s*(\d{1,3})', section_title_raw)
            if appendix_match:
                is_korean_chapter = False
                is_appendix = True
                appendix_num = int(appendix_match.group(1))
                section_number = f"ì²¨ë¶€ {appendix_num}"  # Normalize format
                section_number_list = [999, appendix_num]  # Use special prefix for appendix
                
                # Only process if we're collecting and have passed chapter 6
                if collect and expected_chapter_num and expected_chapter_num >= 6:
                    in_appendix_mode = True
                    last_appendix_num = appendix_num
                    expecting_next_subsection = False  # No subsections expected after appendix
                    print(f"âœ… Found {section_number} (appendix mode activated)")
                else:
                    print(f"â­ï¸ Skipping {section_number} - not in chapter 6+ or not collecting")
                    continue
            else:
                # numeric heading
                is_korean_chapter = False
                is_appendix = False
                sec_num_match = re.match(r'(\d{1,2}(?:\.\d{1,2}){0,3})', section_title_raw)
                if not sec_num_match:
                    # skip if not a valid heading
                    continue
                section_number = sec_num_match.group(1)
                section_number_list = parse_number(section_number)
            
            # Special handling after ì œ N ì¥
            if expecting_next_subsection and expected_chapter_num is not None:
                # We're expecting N.1 after ì œ N ì¥
                if len(section_number_list) >= 2 and section_number_list[0] == expected_chapter_num and section_number_list[1] == 1:
                    expecting_next_subsection = False
                    last_valid_number = section_number_list
                    current_chapter_sections.add(section_number)
                    print(f"âœ… Found expected {section_number} after ì œ {expected_chapter_num} ì¥")
                else:
                    # Check if this is a valid subsection of current chapter
                    if len(section_number_list) >= 1 and section_number_list[0] == expected_chapter_num:
                        # It's a subsection of the current chapter, check if it's in order
                        if is_out_of_order(section_number_list, expected_chapter_num, current_chapter_sections):
                            # Out of order section - merge with previous
                            if merge_out_of_order_section(section_title_raw, section_content, section_number):
                                continue
                        else:
                            expecting_next_subsection = False
                            last_valid_number = section_number_list
                            current_chapter_sections.add(section_number)
                            print(f"âœ… Found subsection {section_number} of ì œ {expected_chapter_num} ì¥")
                    else:
                        print(f"âš ï¸ Expected {expected_chapter_num}.x subsection but found {section_number}, skipping")
                        continue
            
            # Check if section is out of order within current chapter
            if expected_chapter_num is not None and len(section_number_list) >= 1:
                if section_number_list[0] == expected_chapter_num:
                    if is_out_of_order(section_number_list, expected_chapter_num, current_chapter_sections):
                        if merge_out_of_order_section(section_title_raw, section_content, section_number):
                            continue
            
            # Skip 3.x sections with >2 levels as before
            if section_number_list[0] == 3 and len(section_number_list) > 2:
                content_hash = hash_text(section_content)
                if sections and content_hash not in merged_content_cache:
                    print(f"ğŸ“ Merging skipped 3.x section into previous: {section_title_raw}")
                    sections[-1]["content"] += "\n\n" + section_content
                    merged_content_cache.add(content_hash)
                else:
                    print(f"â­ï¸ Duplicate 3.x content skipped: {section_title_raw}")
                continue

        # Only process if we've started collecting
        if not collect:
            continue

        # For numeric sections and appendices, check ordering (non-out-of-order sections)
        if not is_korean_chapter and last_valid_number is not None and not is_appendix:
            # Get current chapter number if we're in a chapter
            current_chapter_num = None
            if expected_chapter_num is not None:
                current_chapter_num = expected_chapter_num
            
            if not is_strictly_next(last_valid_number, section_number_list, current_chapter_num, current_chapter_sections):
                # Handle out-of-order sections
                if section_title_counts.get(section_number, 0) > 1:
                    print(f"â­ï¸ Out-of-order heading {section_number} appears multiple times, skipping early merge.")
                    continue

                if section_number in seen_section_numbers:
                    print(f"â­ï¸ Known section {section_number} already processed, skipping merge.")
                    continue

                if section_number in merged_out_of_order:
                    print(f"â­ï¸ Already merged out-of-order heading {section_number}, skipping repeat.")
                    continue

                print(f"ğŸ“ Merging out-of-order heading {section_number} (page {page_idx + 1}) into previous section")
                merged_text = section_title_raw + "\n" + section_content
                if sections:
                    merged_hash = hash_text(merged_text)
                    if merged_hash not in merged_content_cache:
                        sections[-1]["content"] += "\n\n" + merged_text
                        merged_out_of_order.add(section_number)
                        merged_content_cache.add(merged_hash)
                        seen_section_numbers.add(section_number)
                continue
        
        # For appendices, check sequence
        if is_appendix and in_appendix_mode:
            if appendix_num != last_appendix_num + 1 and last_appendix_num > 0:
                print(f"âš ï¸ Appendix out of order: expected ì²¨ë¶€ {last_appendix_num + 1} but found ì²¨ë¶€ {appendix_num}")
                # But still process it
            last_appendix_num = appendix_num

        # Mark as seen and update last valid number
        seen_section_numbers.add(section_number)
        if not is_korean_chapter and not is_appendix:
            last_valid_number = section_number_list
            current_chapter_sections.add(section_number)

        # Build hierarchy
        if is_korean_chapter:
            depth = 0  # Korean chapters are top level
            section_hierarchy[0] = (section_number, section_title_raw)
            # Clear lower levels when starting new chapter
            for d in list(section_hierarchy):
                if d > 0:
                    del section_hierarchy[d]
        elif is_appendix:
            depth = 0  # Appendices are also top level
            section_hierarchy[0] = (section_number, section_title_raw)
            # Clear lower levels when starting new appendix
            for d in list(section_hierarchy):
                if d > 0:
                    del section_hierarchy[d]
        else:
            depth = section_number.count(".") + 1
            section_hierarchy[depth] = (section_number, section_title_raw)
            for d in list(section_hierarchy):
                if d > depth:
                    del section_hierarchy[d]
        
        display_title = section_title_raw
        if depth > 1:
            parent = section_hierarchy.get(depth - 1)
            display_title = parent[1] if parent else section_title_raw

        # Add final section
        start_page = page_idx + 1
        if len(section_content.strip()) < 1:
            # Keep headings with depth 0, 1 or 2 even if they're empty (including appendices)
            if not is_korean_chapter and not is_appendix and len(section_number_list) >= 3:
                print(f"â­ï¸ Skipping empty deep-level section {section_number}")
                continue
            else:
                print(f"ğŸª§ Keeping empty heading: {section_number}")
        
        # Build hierarchical tree for this section
        section_tree = []
        
        # Add document title if available
        if document_title:
            section_tree.append(f"ğŸ“„ {document_title}")
        
        # Add all parent sections in hierarchy
        for level in sorted(section_hierarchy.keys()):
            if level <= depth:
                _, title = section_hierarchy[level]
                indent = "  " * level
                if level == depth:
                    # Current section - use filled arrow
                    section_tree.append(f"{indent}â–¶ {title}")
                else:
                    # Parent section - use hollow arrow
                    section_tree.append(f"{indent}â–· {title}")
        
        # Create the hierarchical tree header
        tree_header = "\n".join(section_tree)
        
        # Prepend the tree to the section content
        section_content = tree_header + "\n" + "â”€" * 50 + "\n\n" + section_title_raw + "\n" + section_content.strip()
        
        print(f"âœ… Added section: {section_number} (page {start_page})")
        sections.append({
            "title": display_title,
            "section_id": section_number,
            "content": section_content.strip(),
            "start_page": start_page,
            "section_hierarchy": dict(section_hierarchy),
            "document_title": document_title
        })

    return sections


# Backward compatibility aliases
clean_text_by_fixed_margins_with_html_tables = clean_text_by_fixed_margins_with_tables
extract_html_tables = extract_text_tables = True