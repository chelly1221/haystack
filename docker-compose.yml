services:
  qdrant:
    image: qdrant/qdrant
    container_name: qdrant
    ports:
      - "6333:6333"
    volumes:
      - ./qdrant_data:/qdrant/storage
    restart: always

  vllm-server:
    image: vllm/vllm-openai:v0.6.4.post1
    container_name: vllm-server
    ports:
      - "8080:8080"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./models:/models
    environment:
      - NVIDIA_VISIBLE_DEVICES=0
    command: >
      --model /models/A.X-3.1-Light
      --host 0.0.0.0
      --port 8080
      --gpu-memory-utilization 0.98
      --dtype auto
      --enable-auto-tool-choice
      --tool-call-parser hermes
      --temperature 0.7
      --top-p 0.9
      --top-k 50
      --repetition-penalty 1.1
      --max-tokens 4096
      --frequency-penalty 0.1
      --presence-penalty 0.1
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: always

  haystack-app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: haystack-app
    ports:
      - "8001:8001"
    volumes:
      - .:/app
      - ./models:/app/models
      - ./pdfs:/app/pdfs
      - ./data:/app/data
      - ./uploads:/app/uploads
    env_file:
      - .env
    environment:
      - HAYSTACK_ENV=production
      - NVIDIA_VISIBLE_DEVICES=1
      - VLLM_API_BASE=http://192.168.10.101:8080 
    depends_on:
      - qdrant
      - vllm-server
    runtime: nvidia
    restart: always